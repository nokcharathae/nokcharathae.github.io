---
title: NeRF Paper Review
date: 2022-11-03 00:00:42
categories:
- Review
tags:
comments: true
---


[Paper Review] [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://arxiv.org/pdf/2003.08934.pdf){:target="_blank"}

<!-- more -->

# ABSTRACT
본 논문은 몇 장의 사진을 사용하여 continuous volumetric scene function을 최적화하고 복잡한 scene의 novel view를 합성하기 위한 새로운 방법을 제시합니다. 알고리즘의 **입력**은 단일 연속 5D 좌표(**공간 위치(x, y, z) 및 바라보는 방향((θ, φ)**)이고, **출력**은 해당 공간 위치에서 **volume density**와 **view-dependent emitted radiance(color)**로 이를 **fully-connected deep network**를 통해 학습하여 렌더링합니다. 카메라 ray를 따라 **5D 좌표를 쿼리**하여 view를 합성하고 **volume rendering** 기술을 사용하여 출력 색상과 밀도를 이미지에 투영합니다. 본 논문은 scene의 사실적인 novel view를 렌더링하기 위해 **neural radiance field**를 효과적으로 최적화하는 방법을 설명하고, 기존의 neural rendering 및 view synthesis의 작업을 능가하는 결과를 보여줍니다. 
* * * 

<br>
# INTRODUCTION
## Neural 3D Represenation

- Traditional **Explicit** Representations ⇒ Discrete
- **Implicit** Neural Representation ⇒ Continuous

![[https://www.cvlibs.net/publications/Peng2020ECCV_slides.pdf](https://www.cvlibs.net/publications/Peng2020ECCV_slides.pdf)](/assets/images/Image_NeRF/3D Representation.png)
*[<span style='color:#adadad'>https://www.cvlibs.net/publications/Peng2020ECCV_slides.pdf</span>](https://www.cvlibs.net/publications/Peng2020ECCV_slides.pdf){:target="_blank"}*
{: .text-center }


3D Representation은 3D Object를 나타내는 방법입니다.  기존에는 직접 비싼 3D Scanner를 사용하여 실제 물체를 렌더링 시키거나 사진 측량 방법을 통해 
이미지로부터 Voxel이나 Point Cloud, Mesh 형태인 Explicit 방식의 3D Object를 얻어내곤 했습니다. 
그러나 이러한 방식은 복잡한 Geometry의 사실적인 장면을 재현할 수 없었습니다.

이에 최근 컴퓨터 비전의 연구 방향은 3D 공간 위치를 implicit representation으로 직접 매핑하는 deep network를 최적화하여 해당 weight를 통해 연속적인 3D 모양 집합으로 implicit하게 표현하는 것이었습니다
그러나 지금까지 이러한 모델은 일반적으로 explicit한 모델의 성능과 크게 다를 바가 없거나, 합성 3D 형상 데이터 세트에서 얻은 실제 3D geometry가 필요하다는 한계가 있었습니다.



# 참고
* https://keras.io/examples/vision/nerf/