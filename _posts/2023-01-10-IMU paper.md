---
title: '[Review] Mobile. Egocentric Human Body Motion Reconstruction Using Only Eyeglasses-mounted Cameras and a Few Body-worn Inertial Sensors Paper Review'
date: 2023-01-10 00:00:01
categories:
- Review
tags: IMU
comments: true
use_math: true
---


[Paper Review] [Mobile. Egocentric Human Body Motion Reconstruction Using Only Eyeglasses-mounted Cameras and a Few Body-worn Inertial Sensors](https://par.nsf.gov/servlets/purl/10300348)

<!-- more -->

# Abstract

- 사용자들이 시공간 구애 없이 이용할 수 있는 편리한 **telepresence(원격 현장) 시스템**을 구상
- 머리에 착용하는 장치에 내장된 **카메라**와 손목과 발목에 착용하는 **IMU(관성 측정 장치)**를 통해 사람의 **동적 3D 캡처**를 위한 **독립형 실시간 시스템** 제시
- 본 논문의 프로토타입 시스템은 **학습 기반 포즈 추정**을 통해 착용자의 동작을 자기 중심적으로 재구성
- **시각(visual) 및 관성(inertial) 센서**의 입력을 융합 → 1) head-worn views의 일관성 없는 팔다리의 가시성 2) 희소 IMU의 모호성과 같은 **문제**를 극복
- 추정된 포즈는 prescanned surface model로 지속적으로 **리타겟팅**되어 high-fidelity(충실도)가 높은 **3D reconstruction**을 초래
- 다양한 인체 움직임을 재구성하여 시스템을 시연하고 실시간으로 실행되는 **visual-inertial 학습 기반 방**법이 시각 전용 및 관성 전용 접근 방식을 모두 **능가**한다는 것을 보여줌
- [https://sites.google.com/site/youngwooncha/egovip](https://sites.google.com/site/youngwooncha/egovip%EC%97%90%EC%84%9C)

---

# 1. Introduction

- telepresence는 떨어져 있는 공간에서도 원격 사회적 상호 작용을 가능하게 함
- 지금까지의 문제
    - eyeglass-frame form의 문제 :  팔다리의 가시성 떨어짐
    
    ![Untitled](%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/Untitled.png)
    
    - 프레임별 시각적 3D 포즈 추정 방법 : 불완전한 가시성으로 인해 막힌 관절에 대한 신뢰할 수 없는 추정치
    - 외부 카메라 캡처 방법 : 실시간으로 높은 정확도와 성능을 달성, 모든 관절이 보여야 함
    - Joint heatmap estimation methods : 2D 열 지도 내에서 레이블을 지정할 수 없기 때문에 영상 외부에 있는 조인트를 처리할 수 없음 → 경계를 패딩하여 히트맵 크기를 확장하면 와이드 FoV 또는 어안 렌즈의 왜곡이 크기 때문에 높은 3D 조인트 오류가 발생할 가능성

광범위한 수용을 위해 IMU 센서의 수를 줄이는 것 → 

- 시각-관성 융합 접근법은 10개 이상의 신체 착용 센서가 필요
- **eyeglass-frame mounted 카메라 & 손목과 발목의 IMU에만 의존하는 wearable 3D acquisition system 제안**
    1. visibility-aware visual 3D pose network : 가려진 관절을 억제하면서 보이는 관절 추정
    2. online IMU offset calibration method : 부착된 IMU가 있는 팔뚝과 다리 아래에 대해 시간이 지남에 따라 시각적 및 관성 뼈 방향을 정렬하여 관성 측정을 향상
    3. visual-inertial 3D pose network : IMU가 없는 상완과 허벅지의 포즈는 이전 프레임에서 상완골의 시각적 감지뿐만 아니라 상응하는 아래쪽 뼈의 일련의 관성 측정을 사용하여 추정

3D 관절 위치와 3D 뼈 방향을 포함한 전신 포즈는 일부 관절이 이미지에서 벗어나거나 가려진 경우에도 지속적으로 추정되고 시간적으로 일관성을 유지

**기여**

- 희소 가시성과 희소 관성 센서를 모두 처리할 수 있는 최초의 자기 중심적 3D 인간 자세 추정 접근법.
- 모바일 캡처 및 실시간 신체 움직임 추정을 위한 안경 폼 팩터의 작동 중인 독립적인 개념 증명 프로토타입
- 관성 측정뿐만 아니라 공동 가시성 정보가 있는 여러 뷰를 포함하는 최초의 자기 중심적 인간 모션 데이터 세트.

# 2. RELATED WORK

## 2.1. Body Reconstruction

- 모델 매개 변수의 추정은 신체 모델과 이미지 사이의 밀도 높은 대응을 추정하거나 직접적인 체적 추론을 통해 시각적 포즈 추정과 함께 인간 표면을 근사화함
- 최근 얼굴 및 손 포즈 뿐만 아니라 **temporal pose**를 사용하여 실시간 성능의 향상
- 충실도가 높은 geometry는 이미지 실루엣을 맞추거나 천 시뮬레이션을 통해 추정할 수 있음
- 해당 방식은 외부 카메라가 전신을 담을 수 있어야 하지만, egocentric view에서 신체 부위가 가려질 때가 있음

## 2.2. Visual Pose Estimation

- CNN을 사용하여 2D joint heatmap 기반 추정
- CNN 기반 3D joint 추정은 단일 외부 뷰에 대해 실시간으로 상당한 정확도 달성
- Human pose constraint와 occlusion information가 훈련 중에 통합
- 시간이 따른 지속적인 인간 움직임의 경우, RNN 기반 포즈 추정이 일련의 움직임 예측에 대해 유망한 결과
- 해당 방식은 관절 위치를 추정하지만, 전신 자세를 추정하기 위해 시각적 정보만 사용할 때 3D 뼈 방향 추정은 여전히 미해결 문제

## 2.3. Visual Egocentric Pose Estimation

- 몸에 착용한 카메라에 찍힌 자기 중심적 데이터로 고품질 재구성 미해결 → 계측되지 않은 임의의 환경에서 작동하는 재구성 방법이 필요
- 외부 카메라 기반 인간 포즈 추정 방법은 신체의 자기 중심적 관점에 직접 적용할 수 없음
- 신체 움직임은 모션 구조 또는 학습 기반 접근법으로 몸에 착용한 카메라에서 추론 가능
- 신체를 직접 관찰하지 않으면 포즈 추정 정확도가 제한 → 착용자의 신체 대부분을 볼 수 있는 머리 착용형 하향 wide-FoV 카메라로 개선
- 단일 머리 착용 카메라 뷰를 기반으로 한 최근의 방법은 자세 추정 개선에 도달하기 위해 덜 객관적으로 장착된 카메라를 사용 → But, 광범위하게 수용하기에는 너무 눈에 띔
- 아래쪽 몸에 가까운 시야를 사용하는 접근법은 몸에 착용한 카메라에서만 착용자의 전신 자세를 추정의 self-occlusion 및 시야 밖 joint의 문제 미해결

## 2.4. Inertial Pose Estimation

- 인체 자세 추정은 신체 착용 관성 측정 장치(IMU)를 사용하여 수행
- IMU는 빠른 움직임을 포착 및 카메라 뷰에 가려지는 신체 부위를 추적 → But, 시간이 지남에 따라 측정 노이즈와 drift로 인해 어려움을 겪으며 초기 자세에 대한 세심한 보정이 필요
- 센서 착용 수용성을 높이기 위해, 시간적 방향과 가속을 사용하여 IMU의 수를 줄임
- IMU는 팔뚝과 하퇴부에만 착용하며, 사라진 위쪽 팔과 허벅지 방향은 아래쪽 뼈와 위쪽 뼈의 시간적 동작이 높은 상관 관계가 있다고 가정함으로써 추정
- 유사한 측정으로 여러 포즈가 가능할 수 있기 때문에 포즈 모호성 → 미래 프레임 또는 전체 시퀀스와 같은 더 많은 시간적 측정을 사용하여 부분적으로 해결
- 이 문제를 극복하기 위해 시각 및 관성 센서 융합은 IMU와 공동으로 외부 지향 카메라를 활용하여 3D 신체 자세를 계산
- 외부 지향 카메라의 시각적 포즈 추정치는 관성 센서의 가능한 3D 포즈를 제한하고 IMU 측정 노이즈를 완화하는 데 도움 → 지금까지 완전한 신체 가시성을 요구하며, 이는 자기 중심적 관점에서는 거의 달성할 수 없음

# 3. WEARABLE CAPTURE AND EGOCENTRIC DATASET

## 3.1. Eyeglasses and IMUs Prototype

**목표** : 안경, 손목 밴드 및 신발과 같이 일반적으로 착용하는 품목에 센서가 내장된 완전 모바일 텔레프레즌스 시스템을 개발하는 것

**프로토타입** : 안경테 카메라 & 4개 IMU(손목과 발목의 Xsens MTw Awinda)과 밉 러닝 기반 기술

![Untitled](%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/Untitled%201.png)

## 3.2. Egocentric Visual+Inertial Human Pose Dataset

- 프로토타입 헤드셋과 8개의 IMU를 착용한 사용자로 새로운 인간 포즈 데이터 세트를 수집
- 실제 전신 3D 관절은 캡처 스튜디오에서 여러 개의 벽 장착형 카메라를 사용하여 획득

![Untitled](%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/Untitled%202.png)

- training : 22 sequences, evaluation : 9 sequences
- 6 human subjects, for a total of **38k** frames of visual+inertial data
    - visual : 11k개의 실제 이미지를 균일하게 샘플링하고 전체 기록에서 수동으로 필터링. 38k개의 합성 이미지는 의류 및 배경 질감, 헤드기어 변환 등 임의의 증강을 통해 실제 데이터에서 신체 포즈를 사용하여 생성. 각 관절 가시성은 자기 중심 이미지에 투영된 신체 모델의 z buffer를 사용하여 추정되었으며 가시적, 폐색 또는 FoV 외부로 레이블이 지정. 몸통 관절(목, 어깨, 엉덩이)은 뼈 자세 추정을 위한 뿌리 관절로서 필수적인 역할을 하기 때문에 폐색에 관계없이 눈에 보이는 것으로 표시.
    - inertial : 8개 센서의 관성 데이터는 시각 데이터와 동기화 및 보정. 38k 프레임의 실제 IMU 데이터는 정면에서 후면으로 포즈를 미러링하고, 측면에서 시간적으로 부드럽게 포즈 방향을 조정하고, 무작위 가속도 노이즈를 도입하여 증강.

# 4. EGOCENTRIC RECONSTRUCTION METHOD

- **시각-관성 센서**에서 사용할 수 있는 **정보가 희소**하여 전신 자세를 자체적으로 추정할 수 없음
    1. 팔다리 움직임은 신체에 의해 자주 가려지거나 카메라 시야밖에 있기 때문에 보이지 않음
    2. IMU는 팔뚝과 다리 아래쪽에만 착용하므로 팔 위쪽과 허벅지 방향이 없음

→ **visibility-aware visual pose network and a temporally integrated visual and inertial pose network**(가시성 인식 시각적 포즈 네트워크 및 시간적으로 통합된 시각적 및 관성적 포즈 네트워크) 제안

![Untitled](%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/Untitled%203.png)

**Stage1 |** [visibility-aware 3D joint detector network](https://www.notion.so/Paper-Review-Mobile-Egocentric-Human-Body-Motion-Reconstruction-Using-Only-Eyeglasses-mounted-Came-76a631d33df640a5918de1823900010e)는 두 개의 자기 중심적 하향 시야에서 관찰할 수 있는 joint의 3D 위치를 추정하며, [감지된 3D joint는 **VSLAM**](https://www.notion.so/Paper-Review-Mobile-Egocentric-Human-Body-Motion-Reconstruction-Using-Only-Eyeglasses-mounted-Came-76a631d33df640a5918de1823900010e)을 통해 추정된 헤드셋 포즈를 사용하여 world space으로 변환

**Stage2 |** 하부 뼈(팔뚝, 아래쪽 다리)와 상부 뼈(위쪽 팔, 허벅지)의 3D 방향은 각각 [visual-inertial IMU offset calibrator](https://www.notion.so/Paper-Review-Mobile-Egocentric-Human-Body-Motion-Reconstruction-Using-Only-Eyeglasses-mounted-Came-76a631d33df640a5918de1823900010e)와 [temporal visual-inertial orientation network](https://www.notion.so/Paper-Review-Mobile-Egocentric-Human-Body-Motion-Reconstruction-Using-Only-Eyeglasses-mounted-Came-76a631d33df640a5918de1823900010e)를 사용하여 추정

**Stage3 |** [parametric body model](https://www.notion.so/Paper-Review-Mobile-Egocentric-Human-Body-Motion-Reconstruction-Using-Only-Eyeglasses-mounted-Came-76a631d33df640a5918de1823900010e)의 모양과 포즈는 Stage2에서 추정된 전신 3D joint 위치와 방향을 사용하여 설정

<aside>
💡 **VSLAM(Visual** **Simultaneous localization and mapping)**
: 시각 기반의 동시적 위치추정 및 지도작성
주변 환경에 대하여 지도를 작성하면서 동시에 이 환경을 기준으로 카메라의 위치와 방향을 계산하는 과정

</aside>

## 4.1. 3D Body Representation

### **SMPL**(skinned multi-person linear model) parametric body model

: 체형과 포즈를 나타내기 위한 모델

![Untitled](%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/Untitled%204.png)

- 10개의 **shape parameters** $**\beta**$, 24x3=72개의 **pose parameters $\theta$**로 구성
- linear blend skinning을 통해 6480개의 vertice를 가진 **triangular mesh** $\mathcal{M}(\theta,\beta)$을 변형
    
    <aside>
    💡 linear blend skinning : 어떤 3차원 물체를 컴퓨터 그래픽으로 형상화시킬 때에 사람의 뼈 구조(skeleton structure)로부터 Mesh를 만드는 작업
    
    </aside>
    

### Bone representation

![Untitled](%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/Untitled%205.png)

![Untitled](%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/Untitled%206.png)

- 뼈 $***i***$는 두 개의 연결된 **joint와 transform**에 의해 정의
    - local bone rotation 집합 표현 $**\theta$** 대신, **global transform 집합** $T^{M}\in \mathbb{R}^{4\times 4}$으로 정의된 동등한 뼈 표현 사용
    - **Body Mesh space**는 $M$으로, **Skeleton space**는 $S$로 표현
    
- global space의 skeletal bone transformation $T_{i}^{S}\in \mathbb{R}^{4\times 4}$ 을 pose in the skeleteon으로 표현
    
    $$
    \begin{equation*} T_{i}^{S}=\begin{bmatrix} R_{i}^{S} & J_{p(i)}\\ 0 & 1 \end{bmatrix} \tag{1} \end{equation*}
    $$
    
    - $R^{S}=[s_{x},s_{y},s_{z}]\in \mathbb{R}^{3\times 3}$ : bone rotation
    - $J_{p}$ : base joint position
    
- $R^{S}$의 열 벡터는 뼈의 **3D 축**을 형성
    - $s_{y}=R^{[:,2]}$ : base(parent)에서 tip(child) 까지 뼈 방향 $d_{i}$
        
        $$
        d_{i}=(J_{i}-J_{p(t)})/(\Vert J_{i}-J_{p(i)}\Vert)
        $$
        
    - 회전으로부터 계산된 뼈의 방향
    
    $$
    \begin{equation*} d_{i}=d(R_{i})=R_{i}^{[:,2]} \tag{2} \end{equation*}
    $$
    
- **pose parameter** $T_{i}^{M}$는 $T_{i}^{S}$으로 직접 계산
    
    $$
    \begin{equation*} T_{i}^{M}=T_{i}^{S}(T_{i,0}^{S})^{-1} \tag{3} \end{equation*}
    $$
    
    - $T_{i,0}^{S}$ : bind pose matrix로 **좌표 프레임 매핑** $\mathcal{F}^{M}\mapsto \mathcal{F}^{S}$
    - $T_{i,0}^{S}$ 는 **rest pose의 joint 위치 $J_{0}$** 를 사용하여 계산되며, $**\beta**$가 변경될 때만 업데이트
    - $J_{0}$ : $(T^{M})^{-1}(J)$
    - $J_{0}$는 shaped vertice에서 **joint regressor** $\mathcal{J}$에 의해 설명
    

### Estimate the body shape $\beta$

- $E_{shape}$을 **최소화**하여 unposed된 joint $J_{0}$를 사용하여 신체 형태 $\beta$를 추정
    
    $$
    \begin{equation*} E_{shape}=\sum_{i=1}^{K}\left\Vert\left(T_{i}^{M}\right)^{-1}\left(J_{i}\right)-\mathcal{J}_{i}\left(\mathcal{M}_{0}+\mathcal{B}_{s}(\beta)\right)\right\Vert_{2}^{2}+w_{s}\Vert\beta\Vert_{2}^{2} \tag{4} \end{equation*}
    $$
    
    - $w_{s}=0.001$ : weight for the regularization term
    - $K = 13$ : number of joint
    - $\mathcal{M}_{0}$ : mean shape → reshape vertices
    - $\mathcal{B}_{s}(\beta)$ : linear blend shapes → reshape vertices

### Stage1

---

## 4.2. V**isibility-Aware 3D Joint Detection Network**

- 카메라로 얻은 joint visibility 정보를 통해 신뢰할 수 없는 joint를 **거부**하고 관찰 가능한 관절만 추정
    - **intput** : $m\times m$ egocentric images ($m=320$)
    - ground truth binary visibility $v^{gt}$ : **visible** joint는 **1**로, **invisible** joint는 **0**으로 설정
- 머리에 착용하는 와이드 FoV 카메라 이미지에서 하체 관절은 상체 관절보다 상당히 작게 나타남

### Network Structure for the 3D Joint Detector

![Untitled](%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/Untitled%207.png)

- 2D 인간 포즈 추정에 사용되는 ***Stacked Hourglass* 아키텍처**를 3D joint 추정 네트워크로 **확장**
- **DSNT 회귀 모듈**을 사용하여 히트맵에서 2D 좌표를 추정 → 효율성 증가
    
    <aside>
    💡 DSNT : differentiable spatial to numerical transform
    입력 이미지의 **관심 지점에 대한 수치 좌표를 추론**하는 딥 러닝 접근법 연구로, 기존 convolutional neural network-based 솔루션은 heatmap matching 접근 방식 or fully connected output layer 좌표 조정 회귀이다.
    DSNT 계층은 훈련 가능한 매개 변수를 추가하지 않고 완전히 미분할 수 있으며 좋은 공간 일반화를 나타낸다. 히트맵 매칭과 달리 DSNT는 낮은 히트맵 해상도에서 잘 작동하므로 기존의 광범위한 fully convolutional architecture의 출력 레이어로 사용할 수 있다. 결과적으로 DSNT는 기존 기술에 비해 추론 속도와 예측 정확도 사이의 더 나은 균형을 제공한다.
    
    ![Untitled](%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/Untitled%208.png)
    
    </aside>
    

<aside>
<img src="%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/%25ED%2588%25AC%25EB%25AA%2585.png" alt="%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/%25ED%2588%25AC%25EB%25AA%2585.png" width="40px" /> **1. Hourglass 모듈은 첫 번째 $K$ 채널의 heatmap $*H*$와 마지막 $K$ 채널의 inverse depthmap $*D*$ 추론**

</aside>

- **Heatmap** $H\in \mathbb{R}^{(m/4)\times(m/4)\times K}$
    - Softmax 레이어에 의해 confidence map $*V*$로 정규화
- **Inverse Depthmap** $D\in \mathbb{R}^{(m/4)\times(m/4)\times K}$
    - joint에 대한 정규화된 inverse depth 값을 포함하는 heatmap
    - 정규화된 inverse depth 값 : $(d_{max}-d)/d_{max}$
        - $d_{max}=2$(meter)는 최대 깊이
        - 카메라에 가까운 거리에는 더 높은 값, 더 먼 거리에는 near-zero 값 할당

<aside>
<img src="%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/%25ED%2588%25AC%25EB%25AA%2585.png" alt="%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/%25ED%2588%25AC%25EB%25AA%2585.png" width="40px" /> **2. 회귀 모듈은 $H$에서 정규화된 confidence map *$V$*로부터 $2D$ 좌표** $\mathbf{p}$**를 출력**

</aside>

- $*V*$는 X 및 Y 좌표 행렬의 내적에 의해 $2D$ 좌표 $\mathbf{p}$로 변환
- 신뢰 $\tilde{v}$와 깊이 $*d*$는 각각 $*V*$와 $*D*$로 추정된 **$\mathbf{p}=(x,y)$** 좌표에서 읽힘
- 신뢰 $\tilde{v}$가 충분히 클 때 좌표 $\mathbf{p}$가 유효하고 가시성 $*v*$는 1로 설정, 그렇지 않으면 0으로 설정

<aside>
<img src="%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/%25ED%2588%25AC%25EB%25AA%2585.png" alt="%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/%25ED%2588%25AC%25EB%25AA%2585.png" width="40px" /> **3. 출력으로서 연결된 $*H*$와 $*D*$는 다음 스테이지의 입력으로 전파**

</aside>

- raw inverse depth 판독치는 다시 깊이 $*d*$(meter)로 변환

<aside>
<img src="%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/%25ED%2588%25AC%25EB%25AA%2585.png" alt="%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/%25ED%2588%25AC%25EB%25AA%2585.png" width="40px" /> **4. 단일 입력 이미지가 주어지면, 4단계 네트워크는 $3D$ joint 좌표가 계산되는** $\mathbf{p}$**,$D,V$를 출력**

</aside>

- $3D$ **joint 위치**는 카메라 calibration matrix를 사용한 **back-projecting$(x,y,d)$**으로 계산

### Network **Loss function**

$$
\mathscr{L}_{joint_{-}net}= \mathscr{L}_{DSNT}+\mathscr{L}_{V}+\mathscr{L}_{D}
$$

- $**\mathscr{L}_{DSNT}$ : regression loss** → $v^{gt}=1$
    
    $$
    \begin{equation*} \mathscr{L}_{DSNT}=\sum_{i=1}^{K}v_{i}^{gt}\cdot[\Vert \mathbf{p}_{i}^{gt}-\mathbf{p}_{i}\Vert _{2}^{2}+\mathcal{D}(V_{i}\Vert\mathcal{N}(\mathbf{p}_{i}^{gt},\sigma I_{2}))] \tag{5} \end{equation*}
    $$
    
    - $\mathcal{N}(\mu,\sigma)$ : 2D Gaussian map drawn at μ  with standard deviation σ(σ=1  for training)
    - $*\mathcal{D}(\cdot\Vert \cdot)$ : $H$*가 2D Gaussian map과 유사하게 하기 위한 Jensen-Shannon 발산

- $**\mathscr{L}_{V}$ : invisibility loss** → $v^{gt}=0$
    
    $$
    \begin{equation*} \mathscr{L}_{V}=\sum_{i=1}(1-v_{i}^{gt})\cdot\Vert H_{i}\Vert _{2}^{2} \tag{6} \end{equation*}
    $$
    
    - 보이지 않는 joint에 대해 $*H*$를 제로 히트맵으로 억제
    - $V$에서 균일한 분포를 강화, 보이지 않는 관절에 대해 신뢰 값이 더 작아지도록 장려
    
- .$\mathscr{L}_{D}$ **: depth loss** → $v^{gt}=1$
    
    $$
    \begin{equation*} \mathscr{L}_{D}=\sum_{i=1}^{K}v_{i}^{gt}\cdot\Vert \mathcal{M}(\mathbf{p}_{i}^{gt},\sigma I_{2})\ominus(D_{i}-D_{i}^{gt})\Vert _{2}^{2} \tag{7} \end{equation*}
    $$
    
    - $\mathcal{M}(\mu,\sigma)$ : 2D binary maskmap drawn at μ with radius σ
    - $\ominus$ : 아다마르 곱 (Hadamard product)으로 같은 크기의 두 행렬의 각 성분을 곱하는 연산
    - 깊이 맵은 **관심 joint 영역**만 훈련 → 마스크 맵을 사용하지 않을 때 깊이 맵 출력이 0이 되는 **과적합을 방지**하기 위해 외부 영역이 변경되지 않도록 함

### The network is trained in multiple stages

- $**2D**$ 레이어는 **[MPII Human Pose Dataset](http://human-pose.mpi-inf.mpg.de/)**에서 low-level texture features를 학습하도록 훈련
- **회귀 손실 $\mathscr{L}_{DSNT}$**만 훈련에 사용되고 visibility는 무시
- 네트워크는 전체 손실 함수 $\mathscr{L}_{joint_{-}net}$로 **intermediate supervision**이 적용되어 훈련
- 오른쪽 카메라 이미지를 뒤집어 출력 joint 좌표를 뒤로 뒤집어, 왼쪽 카메라 이미지와 동일한 네트워크를 사용하기 위해 두 개의 하향 카메라 뷰 사이의 대칭 활용 → 두 뷰 모두에 대한 교육 및 런타임에서 **단일 네트워크**를 사용할 수 있게 함

## 4.3. Temporally, Multi-view Consistent Joint Estimation

- 3D joint는 좌우 하향 카메라 뷰에서 **독립적**으로 감지, camera clibration matrices를 사용하여 **단일 3D 공간으로 재투영**
- **잘못된 감지**로 인해 상대 joint와 일치하지 않는 joint의 결과가 **multi-view-consistent와** **시간적으로 일관되도록** 필터링됨
    1. joint의 raw detection은 뼈 방향 $d_{i}$가 시간적으로 일관성이 없는 경우 걸러지며, 이는 프레임 간에 30**°** 이상의 변화로 정의
    2. **필터링된 측정**은 가중치 합계를 **최소화**하여 multi-view-consistent 및 시간적으로 일관된 joint 위치 $X$를 추정하는데 사용
    
    $$
    E_{proj}+w_{d}E_{dep}+w_{l}E_{len}+w_{t}E_{temp}
    $$
    
    - 가중치 설정
        - $w_{d}=1,w_{l}=0,w_{t}=10$ : 목, 엉덩이 및 어깨를 포함한 몸통 joint
        - $w_{d}= 2, w_{l}=2,w_{t}=1$ : 팔 joint
        - $w_{d}=1,w_{l}=5,w_{t}=2$ : 다리 joint
    - **Projection cost** : $E_{proj} = \Sigma_{c=1}^{C}\Vert \mathbf{p}_{c}-P_{c}\cdot X\Vert _{2}$
        - $C$ : number of views
        - $\mathbf{p}_{c}$ : 2D location measurement in camera image *c*
        - $P_{c}$ : camera c’s projection matrix
    - **Depth cost** : $E_{dep} = \Sigma_{c=1}^{C}\Vert d_{c}-T_{c}^{[3,:]}\cdot X\Vert _{2}$
        - $d_{c}$ : depth measurement in camera *c*
        - $T_{c}^{[3,:]}$ : third row of the extrinsic matrix of camera *c*
    - **Bone length consistency** : $E_{len} = \Vert X_{l}-\Vert X_{p}-X\Vert _{2}\Vert _{2}$
        - 골격 길이는 초기화부터 새로운 검출 측정을 통해 평균화까지 시간이 지남에 따라 유지
        - 초기 골격 길이는 정지 자세 시 신체 모델에서 가져와 모델과 검출된 척추 길이 사이의 비율로 스케일링
        - $X_{p}$ : parent joint’s position
        - $X_{l}$ : bone length of joint $X$
    - **Temporal smoothness cost** : $E_{temp} = \Vert X_{t-1}-X\Vert _{2}$
        - $X_{t-1}$ : joint position in the previous frame
- 헤드셋 공간 3D joint 위치 ***$X$* → VSLAM**을 통해 획득한 **현재 추정된 headset pose** → 3D 세계 공간 joint 위치  $*J*$로 변환
- 전체 프로세스는 직접적인 triangulation을 사용할 때보다 **정확도 향상**

![Untitled](%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/Untitled%209.png)

![Untitled](%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/Untitled%2010.png)

### Stage2

---

## 4.4. Visual-Inertial Alignment

- 사람의 자세는 해당 뼈의 방향을 추적하는 **신체 착용 관성 센서(IMU)**로 추정
- IMU는 일반적으로 특정 초기 포즈를 사용하여 보정 → 약간 잘못 정렬된 신체 착용 IMU도 시각적 관성 일관된 포즈 추정을 방해하여 부정확한 결과를 초래

### Coordinate frame transformations

![Untitled](%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/Untitled%2011.png)

(a) 사전 정의된 wear pose를 나타내는 스켈레톤 공간 $R^{SI}$에 대한 관성 센서 회전

(b) 잘못 정렬된 IMU를 보정하는 데 사용되는 IMU 회전 오프셋 $R^{W}$

- 시간에 따라 아래쪽 뼈 방향의 Visual-Inertial쌍을 사용하여 IMU 회전 오프셋 $R^{W}$를 추정함으로써 **부정확성** 확인

### Computing the lower bone

- 시간 단계 $t$에 따른 뼈 회전 $R_{t}^{S}$은 하부 뼈에 장착된 IMU를 통해 계산
    
    $$
    \begin{equation*} R_{t}^{S}=R^{W}\cdot R_{t}^{I}\cdot(R^{SI})^{-1}\cdot R_{0}^{S} \tag{8} \end{equation*}
    $$
    
    - 가시성 관계 X
    - $R_{t}^{I}$ : orientation read from the Inertial sensor at time $t$
    - $R_{0}^{S}$ : rotation from $T_{0}^{S}$
    - $(R^{SI})^{-1}$ : $\mathcal{F}^{S}\rightarrow \mathcal{F}^{I}$로 프레임 매핑
- Inertial lower bone direction
    
    $$
    \begin{equation*}
    d_{t}^{I}=R_{t}^{I}\cdot(R^{SI})^{-1}\cdot d(R_{0}^{S})
    \tag{9}
    \end{equation*}
    $$
    
    - $d(R_{0}^{S})$  : bone direction in the rest pose

### Estimate IMU rotation offset

- IMU 회전 오프셋 $R^{W}$은 동일한 뼈의 시각적 검출기에서 측정을 사용할 수 있을 때마다 업데이트
    - prior bone direction $d(R_{1}^{S}), \ldots,d(R_{t}^{S})$ **=** visual bone direction **$d_{1}^{V}, \ldots,d_{t}^{V}$**
- 센서가 정확히 지정된 위치와 방향으로 착용된 경우 $R^{W}=I_{3}$
- $R^{W}$은 **최소 제곱 문제**를 해결하여 일련의 Visual $d^{V}$ 및 Inertial $d^{I}$ 방향으로부터 추정할 수 있음:
    
    $$
    \begin{equation*}
    \min_{R^{W}}\sum_{t}\Vert d_{t}^{V}-R^{W}\cdot d_{t}^{I}\Vert _{2}^{2}
    \tag{10}
    \end{equation*}
    $$
    
    - 모든 $(d^{I},\ d^{V})$ 쌍에 대해 위 식을 계산하는 것은 계산 집약적
    - 온라인 k-d 트리 구조를 가진 Algorithm 1에 설명된 온라인 k-평균 알고리즘을 사용하여 visual-inertial 쌍을 그룹화하고 $R^{W}$를 업데이트
        
        ![Untitled](%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/Untitled%2012.png)
        
        - At runtime, k-d 트리에서 고정된 k=200개의 클러스터 쌍 유지
        - 샘플링 전략은 군집 간 거리를 최대화 → 군집의 균일한 분포를 선호, 선형 표본의 수를 최소화

## 4.5. Temporal Visual-Inertial Orientation Network

- 위쪽 팔 및 허벅지(상부)의 **방향**은 팔뚝 및 아래쪽 다리(하부)의 **움직임**과 높은 **상관 관계 →** 상부 뼈 움직임은 여러 방향 가능 →  상부 뼈의 **시각적 관찰**을 사용
    - 센서로 계측된 하부 뼈(***i***)와 계측되지 않은 상부 뼈(***u***) 구별
    - 보정된 팔뚝과 아래쪽 다리의 방향 $R_{\mathbf{i}}^{S}$은 IMU offset matrix $R_{\mathbf{i}}^{W}$으로 계산
- ***H*eading reset :** $a_{\mathbf{i}}^{S}=R_{\mathbf{i}}^{H}\cdot a_{\mathbf{i}}^{I}$ 는 $R^{W}$에서 계산된 위쪽 방향을 따라 회전
    - $a_{\mathbf{i}}^{I}$ : raw acceleration
    - $R^{H}$ : IMU acceleration offset matrix

### Estimate upper bone

- 식 $(2)$에 따라 $R_{\mathbf{i}}^{S}$, 하부 뼈에 대한 $a_{\mathbf{i}}^{S}$, 시각적인 상부 뼈 빙향 $d_{\mathbf{u}}^{V}$의 가용성을 통해 비계측 상부 뼈 $R_{\mathbf{u}}^{S}$를 추정
    - 신체 방향에 불변하기 위해, $R_{\mathbf{i}}^{S}$, $a_{\mathbf{i}}^{S}$, $d_{\mathbf{u}}^{V}$는 시간 $t$에서 root joint(엉덩이 중심) 방향 $R_{root}^{S}$에 대해 정규화:
    
    $$
    \begin{equation*} R^{N}(t)=(R_{root}^{S}(t))^{-1}\cdot R_{\mathbf{i}}^{S}(t) \tag{11} \end{equation*}
    $$
    
    - $a_{\mathbf{i}}^{S}\rightarrow a^{N}$과  $d_{\mathbf{u}}^{V}\rightarrow d^{N}$ 비슷하게 정규화
        
        N : Normalized torso space
        

### Temporal visual-inertial orientation network architecture

![Temporal visual-inertial orientation network architecture. 일련의 visual-inertial input feature vectors x를 사용하여 비계측 방향 y를 추정한다. 모든 레이어는 훈련에서 dropout 0.2를 사용한다. 괄호 안의 숫자는 각 레이어의 출력 치수이다.](%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/Untitled%2013.png)

Temporal visual-inertial orientation network architecture. 일련의 visual-inertial input feature vectors x를 사용하여 비계측 방향 y를 추정한다. 모든 레이어는 훈련에서 dropout 0.2를 사용한다. 괄호 안의 숫자는 각 레이어의 출력 치수이다.

- 일련의 input features $\mathbf{x}=[x_{t-n+1},\ \ldots,x_{t}]$에서 계측되지 않은 골격 방향을 예측하는 함수 $\mathbf{x}\rightarrow y_{t}$을 학습하는 작업
    - LSTM을 능가하는 **Transformer network**를 사용
    - 입력 시퀀스는 마지막  $n=20$ 프레임의 측정으로 구성
- input feature vector
    
    $$
    \begin{equation*}
    x_{t}=[r_{t}, \omega_{t},a_{t},v_{t}\cdot d_{t}]^{T}
    \tag{12}
    \end{equation*}
    $$
    
    - $r_{t}$ : 4개의 입력 뼈에 대해 $[r_{1}^{N}(t),\ \ldots,r_{4}^{N}(t)]^{T}$를 나타냄
    - $\omega_{t}$, $a_{t}$와 $v_{t}\cdot d_{t}$는 비슷하게 정의
    - $r_{i}^{N}$는 벡터화된 $R_{i}^{N}$이고, $\omega_{i}^{N}(t)$는 $R_{i}^{N}(t)$와 $R_{i}^{N}(t-1)$ 사이의 각속도
    - input feature vector는 회전, 속도 및 가속도로 대표되는 하부 뼈 움직임을 통합
    - 상부 뼈 $i$의 관절이 시각적 검출기에 의해 제공되는 경우, $d_{i}^{N}$ 방향이 추가되고 가시성 $v_{i}$가 1로 설정
        - 그렇지 않으면 실험 결과 $v_i=0.1^{-3}$ 과 $d_{i}^{N}=(1,1,1)$ 을 모두 0으로 설정하는 것보다 성능이 더 우수
    - $x_{t}$의 차원은 4개의 IMU 계측 뼈$(r_{t},\omega_{t},a_{t})$와 4개의 비 계측 뼈$(v_{t}\cdot d_{t})$의 경우 $(9+3+3+3)\cdot 4=72$
- output vector
    - 벡터화된 비계측 골격 방향 $y_{t}=[r_{1}^{o}(t),\ \ldots,r_{4}^{o}(t)]^{T}$ 포함
    - $r_{i}^{o}$가 출력 방향 $R_{i}^{o}(t)$로 변경
    - $y_{t}$의 차원은 4개의 상완골과 대퇴골에 대해 (9) $\cdot 4=36$
- **loss function**

$$
\begin{equation*} \mathscr{L}_{bone_{-}net}=\Vert y-y^{gt}\Vert _{2}^{2}+\sum_{j=1}^{4}v_{i}^{gt}\cdot acos(d(R_{i}^{o}),d_{i}^{gt}) \tag{13} \end{equation*}
$$

- orientation loss은 ground truth $y^{gt}$를 사용하여 측정
    - $d(R^{o})$는 식 (2)를 사용하여 계산된 출력 골격 방향을 나타냄
    - ground truth $d^{gt}$ 골격 방향에 의해 패널티가 발생하며, 이는 출력 골격 방향이 시각적 입력 골격 방향과 일치하도록 장려
    - 이 항은 $v^{gt}=1$인 경우에만 계산
- At run-time, 정규화된 몸통 공간의 추정 $R^{o}$는 식 (11)을 사용하여 세계 공간의 $R_{\mathrm{u}}^{S}$로 변환

### Stage3

---

## 4.6. Deformable Body Model Fitting

- 해당 연구의 파이프라인은 관절 위치 $J$와 뼈 회전 $R^{S}$의 전신 형태와 자세 추정
    - 관찰되지 않은 관절 위치는 $R^{S}$의 forward kinematics와 해당 뼈 길이를 사용하여 복구
    - 전신 관절 위치 $J$를 이용하여 식$(4)$ 를 풀어 체형 업데이트

$$
\begin{equation*} E_{shape}=\sum_{i=1}^{K}\left\Vert\left(T_{i}^{M}\right)^{-1}\left(J_{i}\right)-\mathcal{J}_{i}\left(\mathcal{M}_{0}+\mathcal{B}_{s}(\beta)\right)\right\Vert_{2}^{2}+w_{s}\Vert\beta\Vert_{2}^{2} \tag{4} \end{equation*}
$$

- 골격 회전 $R^{S}$는 검출된 시각적 방향 출력 $d^{V}$를 사용하여 추가로 보정(사용 가능 시)
    - 추정된 $R^{S}$는 시간적 일관성이 있지만 모션 또는 가시성에 갑작스러운 변화가 발생할 경우 모션이 over-smoothed → 뼈 방향 $R^{S}$를 $d^{V}$에 더 가깝게 맞춰 해결 → 변화에 대한 더 빠른 반응 촉진
- $d^{V}$를 사용할 수 있는 경우, 보정된 뼈 회전 $\overline{R}^{S}$를 추정:
    
    $$
    \begin{equation*} \bar{R}^{S}=R^{v2v}(d(R^{S}),\ \alpha\cdot d^{V}+(1-\alpha)\cdot d(R^{S}))\cdot R^{S} \tag{14} \end{equation*}
    $$
    
    - $R^{v2v}(v_{1},v_{2})$ : $V_{1}$로부터 $v_{2}$ 벡터로의 회전
    - $a=0.8$
- joint 위치 $\bar{J}$는 $\overline{R}^{S}$를 사용하는 forward kinematics에 의해 업데이트
    - 추정된 joint $\bar{J}$는 시간적으로 일관된 joint 추정을 위해 다음 프레임으로 **전송**
- 포즈 파라미터 $T^{M}$은 식$(1)$과 식$(3)$의  $\overline{R}^{S}$와 $\bar{J}$를 이용하여 추정

$$
\begin{equation*} T_{i}^{S}=\begin{bmatrix} R_{i}^{S} & J_{p(i)}\\ 0 & 1 \end{bmatrix} \tag{1} \end{equation*}
$$

$$
\begin{equation*} T_{i}^{M}=T_{i}^{S}(T_{i,0}^{S})^{-1} \tag{3} \end{equation*}
$$

# 5. **Results and Evaluation**

![Untitled](%5BPaper%20Review%5D%20Mobile,%20Egocentric%20Human%20Body%20Motio%2076a631d33df640a5918de1823900010e/Untitled%2014.png)